# ğŸ”¥ MoEForge

**Fast and memory-efficient Mixture of Experts training library**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.9+](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch 2.0+](https://img.shields.io/badge/pytorch-2.0+-orange.svg)](https://pytorch.org/)

MoEForge is a PyTorch library for training Mixture of Experts (MoE) transformers with state-of-the-art optimizations.

## âœ¨ Features

- ğŸš€ **Token Shuffling** - Efficient vectorized routing without Python loops
- âš¡ **Flash Attention 2** - Memory-efficient attention with O(N) memory
- ğŸ”§ **Custom CUDA Kernels** - MGRMSNORM for fast RMSNorm
- ğŸ¯ **Shared Experts** - DeepSeek-V2 style architecture
- ğŸ“Š **Load Balancing** - Z-loss and auxiliary losses for stable training
- ğŸ”¥ **LigerRMSNorm** - Optional Liger Kernel integration
- ğŸ’¾ **DeepSpeed Ready** - ZeRO optimization support

## ğŸ“¦ Installation

```bash
# Basic installation
pip install moeforge

# With Flash Attention
pip install moeforge[flash-attn]

# With all optimizations
pip install moeforge[full]

# From source
git clone https://github.com/MadrasLe/moeforge.git
cd moeforge
pip install -e .
```

### Optional: Install MGRMSNORM (Custom CUDA Kernel)

```bash
pip install git+https://github.com/MadrasLe/MGRrmsnorm.git
```

## ğŸš€ Quick Start

```python
from moeforge import MoEConfig, MoEModel

# Create configuration
config = MoEConfig(
    vocab_size=32000,
    hidden_dim=1024,
    num_heads=16,
    num_layers=12,
    num_experts=8,
    top_k=2,
    shared_expert=True,  # DeepSeek-style
)

# Build model
model = MoEModel(config)
print(model)  # Shows total and active params

# Forward pass
input_ids = torch.randint(0, 32000, (4, 512))
outputs = model(input_ids)

# With labels for training
outputs = model(input_ids, labels=input_ids)
loss = outputs["loss"]  # Includes LM loss + aux loss
loss.backward()
```

## ğŸ“Š Benchmark Results

Tested on NVIDIA L4 (24GB) with BF16:

| Config | Params Total | Params Active | TPS | Memory |
|--------|-------------|---------------|-----|--------|
| Small | 77M | 58M (75%) | 52,805 | 3.0 GB |
| Medium | 644M | 248M (38%) | 20,534 | 3.2 GB |

With LigerRMSNorm + Flash Attention:
- **66% faster** than naive implementation
- **47% less memory** usage

## ğŸ—ï¸ Architecture

```
MoEModel
â”œâ”€â”€ embed_tokens (Embedding)
â”œâ”€â”€ layers (ModuleList)
â”‚   â””â”€â”€ MoEBlock
â”‚       â”œâ”€â”€ ln1 (RMSNorm)
â”‚       â”œâ”€â”€ attn (OptimizedAttention + RoPE)
â”‚       â”œâ”€â”€ ln2 (RMSNorm)
â”‚       â””â”€â”€ moe (MoELayer)
â”‚           â”œâ”€â”€ gate (Router)
â”‚           â”œâ”€â”€ experts (ModuleList[SwiGLUExpert])
â”‚           â””â”€â”€ shared_ffn (SharedExpert, optional)
â”œâ”€â”€ ln_f (RMSNorm)
â””â”€â”€ lm_head (Linear, weight-tied)
```

## âš™ï¸ Configuration Options

```python
config = MoEConfig(
    # Model size
    vocab_size=32000,
    hidden_dim=1024,
    num_heads=16,
    num_layers=12,
    max_seq_len=2048,
    
    # MoE settings
    num_experts=8,
    top_k=2,
    shared_expert=True,
    capacity_factor=1.25,
    gate_temperature=1.0,
    
    # Regularization
    dropout=0.05,
    load_balance_alpha=0.01,
    z_loss_weight=1e-3,
    
    # Optimizations
    norm_type="auto",  # auto, mgrmsnorm, liger, pytorch
    use_flash_attention=True,
    use_bf16=True,
)
```

## ğŸ”§ Normalization Options

MoEForge supports multiple RMSNorm implementations:

| Type | Speed | Memory | Requires |
|------|-------|--------|----------|
| `mgrmsnorm` | âš¡âš¡âš¡ | Low | CUDA kernel |
| `liger` | âš¡âš¡ | Low | liger-kernel |
| `pytorch` | âš¡ | Medium | None |

```python
from moeforge.layers import get_norm_layer, get_available_norms

# Check available implementations
print(get_available_norms())
# {'mgrmsnorm': True, 'liger': True, 'pytorch': True}

# Get specific implementation
norm = get_norm_layer(1024, norm_type="liger")
```

## ğŸ“ Training with DeepSpeed

```python
import deepspeed
from moeforge import MoEConfig, MoEModel

config = MoEConfig(...)
model = MoEModel(config)

# DeepSpeed config
ds_config = {
    "train_batch_size": 1024,
    "bf16": {"enabled": True},
    "zero_optimization": {"stage": 1},
}

model_engine, optimizer, _, _ = deepspeed.initialize(
    model=model,
    config=ds_config,
)
```

## ğŸ“ Project Structure

```
moeforge/
â”œâ”€â”€ moeforge/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py          # MoEConfig dataclass
â”‚   â”œâ”€â”€ layers/
â”‚   â”‚   â”œâ”€â”€ attention.py   # Flash Attention + RoPE
â”‚   â”‚   â”œâ”€â”€ expert.py      # SwiGLU experts
â”‚   â”‚   â”œâ”€â”€ moe.py         # MoE layer with token shuffling
â”‚   â”‚   â””â”€â”€ normalization.py  # RMSNorm implementations
â”‚   â””â”€â”€ models/
â”‚       â””â”€â”€ transformer.py # Complete MoE model
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ train_moe.py
â”œâ”€â”€ tests/
â”œâ”€â”€ pyproject.toml
â””â”€â”€ README.md
```

## ğŸ¤ Contributing

Contributions welcome! Please read our [Contributing Guide](CONTRIBUTING.md).

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details.

## ğŸ™ Acknowledgments

- [Flash Attention](https://github.com/Dao-AILab/flash-attention)
- [Liger Kernel](https://github.com/linkedin/Liger-Kernel)
- [DeepSeek-V2](https://arxiv.org/abs/2405.04434) for shared expert architecture
- [Mixtral](https://arxiv.org/abs/2401.04088) for MoE design inspiration

## ğŸ“š Citation

```bibtex
@software{moeforge2024,
  author = {Gabriel Yogi},
  title = {MoEForge: Fast Mixture of Experts Training},
  year = {2024},
  url = {https://github.com/MadrasLe/moeforge}
}
```

---

**Made with ğŸ”¥ by [MadrasLe](https://github.com/MadrasLe)**
